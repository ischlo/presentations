---
title: "Downscaling Global Economic Activity Data : A Spatial and Sectorial perspective using Bayesian Hierarchical Modelling"
format: 
    revealjs:
        theme: ["./pres_style.scss",default]
    # pdf : default
    # beamer
author:
  - name: Ivann Schlosser
    email: ivann.schlosser@ouce.ox.co.uk
    url: ischlosser.com
    affiliations:
      - name: Oxford Progamme for Sustainable Infrastructure Systems (OPSIS)
        address: South Parks Road
        postal-code: OX1 3QY
        city: Oxford
bibliography: references.bib
---

# Introduction

## Glossary

**BHM** - Bayesian Hierarchical Model

**MCMC** - Markov Chain Monte Carlo

**PDF** - probability density function

**RV** - random variable

**Marginal Probability** - PDF expressing dependency on a single parameter

**Pycnophilcatic** - mass preserving

------------------------------------------------------------------------

## Where we left off

-   Pycnophylactic condition @tobler1979
-   Rescaling, using Overture POIs and their categories as source for density distribution of activity on the ground.

::: columns
::: {.column width="50%"}
### Pros

-   Simple
-   Efficient
-   Global

### Cons

-   Did not use additional available information : sector level data, regional accounts from local sources, other global layers such as population, Non-Residential built infrastructure etc...
-   Univariate model
-   Hard constraint
-   No flexibility

### Can we do better ?
:::

::: {.column width="50%"}
![](imgs/ezgif.com-animated-gif-maker.gif){fig-align="center"}
:::
:::

## YES

But at a bigger cost.

The methodology was significantly expanded to use **Bayesian Hierarchical Modelling** (BHM) [@vandeschoot2021].

### Methods

#### BHM

::: columns
::: column
### Pros

-   Can handle complex data structures
-   Allows for uncertainty quantification
-   Can incorporate prior knowledge
-   Flexibility
:::

::: column
### Cons

-   Gets computationally demanding
-   Requires **careful** prior selection
-   Sensitive to model specification
-   Flexibility
:::
:::

Bayesian is generally complex : Theory and formalism (Bayes, Markov Chain Monte Carlo, Information theory, Statistics...), software ecosystem (world of it's own), PPLs (Probabilistic Programming Languages)...

As the name suggests, at the core of this method lies the familiar Bayes rule. Let us remind ourselves. If we have two random variables (RV), and we are modelling them together, we might find ourselves asking questions of the nature :

> What is the probability of a joint event, or a conditional event, having observed the outcome of a single one ?

## BHM

This, in practice, takes the form $p(x,z)$. And in such cases, Bayes rule tells us that

::: {.columns layout="[[6,4]]"}
::: column
$$
p(x,z) = p(z) * p(x|z) \\
p(x,z) = p(x) * p(z|x)
$$

We can also rewrite this to link both marginals with the joint one : $$
p(x) = \frac{p(z) * p(x|z)}{p(z|x)}
$$
:::

::: column
![](imgs/venn.png)
:::
:::

Now the most important point in BM, is that we apply such reasoning to the parameters of our model. In other words we are asking the question :

> What is the probability of observing some data from my model, given it has some specified parameters $\mathbf{\theta}.$

## Simple example

Let's say we have some data $\{ X_i \}$ and we model it as $X \sim \mathcal{N(\mu,\sigma)}$. Usually, we would use the data to make an estimate on the values of $(\mu,\sigma)$, with both parameters fixed and computed from the data. But what is we set the parameters to have variablility ? In such a case, our model on the data becomes parametrised as $p_X \equiv p_X(.|\mu,\sigma)$. Formally, let's model the parameters as RVs as well. The values we will draw from our original model will be conditioned on the parameters, which will need to be *specified* at the sampling stage. This is where Bayesian thinking starts, because we defined an additional probability density, associated to our belief on the values of the parameters, say for the mean $\mu \sim \mathcal{N(\tau,\gamma)}$. We have a statistical model defined not only for the data, but for the parameters of our model. The distribution on $\mu$ as well as the one for $X$ form our **prior**, encompassing the best of our knowledge about the observed phenomenon. The parameters $(\tau,\gamma)$ are referred to as **hyper-parameters** and can be used to fine tune our belief on the mean of the data we observe and sample down the way.

### What about observed data ?

::: columns
::: column
Let's say now we have some observed sample data $\{X_i\}$. Having laid a general behaviour for our model in the previous paragraph, we can now turn to it and ask the question :

> "What are the chances of observing the sample $\{X_i\}$ in our model, conditioned to the parameters $\vec{\mathbf{\theta}} = \{\mu, \sigma\}$ ?"

In other words we are looking at $p(\{X_i\}|\mu,\sigma)$, recall Bayes rule from an earlier slide. We call this the **likelihood function**.

Ex : Height distribution in human population
:::

::: column
![](imgs/simple_BHM_model.png)
:::
:::

<!-- end columns -->

## Learning from the data

From this point, the model has to *learn* the best possible parameters, given the observed data and *prior* beliefs that we communicated to it. This step relies on MCMC, sampling from the prior distribution and updating it based on the *likelihood*. At the end of this iterative process, we get the **posterior distribution**, which under the given set of priors and parameters, and for a number of samples, gives us the best belief on the parameters for the model to generate data.

### Posterior distribution

The posterior distribution emerges once we have adapted the prior using the likelihood we measure with respect to observed data. This step is similar to a learning epoch in the training process of Deep Learning.

### MCMC

The method allows us to fine tune the posterior distribution, by sampling synthetic data out of the prior and adapting it to be more similar to the observed data at every new iteration. Markov Chains Monte Carlo is tool that allows us to do this.

### Summary BHM

|                | **Data**                         | **Parameters**        |
|----------------|----------------------------------|-----------------------|
| **Prior**      | $f(.|\theta)$                    | $\pi(\theta)$         |
| **Posterior**  | $f(.|\theta)\pi(\theta|\{X_i\})$ | $\pi(\theta|\{X_i\})$ |
| **Predictive** | $Y \sim f(.|\theta, \{X_i\})$    | $\pi(\theta|\{X_i\})$ |

Where the family of $f$ is a choice of the user that can have a huge impact on the modelling. In our case, the choice was done for $LogNormal$.

## Spatial Economic Activity

### How do we develop a downscaling model with this ?

After this summary, we get in the specific details of our problem. The idea is to embed an econometrics model predicting the fine scale values into this Bayesian framework. On the one hand informing the behavior at the fine spatial scale, dictated by the econometrics model, and controlling that this behavior aligns with our prior knowledge and constraints at the coarse spatial and sector scale through the BHM.

::: r-stack
::: {.columns .fragment}
::: column
### Spatial Hierarchy

![](imgs/spat_hierarchy_plot_2.png)
:::

::: column
### Economic Hierarchy

::: {layout="[[-1], [1], [-1]]"}
![](imgs/sector_levels_plot_light.png)
:::
:::
:::
:::

<!-- end columns -->

## Econometrics modelling

We use a simplified linear model, inspired by (spatial) econometrics[@anselin1988; @redding2024; @zellner1985], to *inform* the bayesian method on how we expect our predictor variables to be linked to the industry level output. We apply this model at the fine resolution to every location that has some non-zero predictor variable.

$$
\mu_{S_i} = \sum_m \omega_m * x_m
$$

where $\omega_m$ are learned parameters and $x_m$ are the proxy variables that we assign to the relevant sectors. The result is a multilinear model, where some variables where masked, based on reasonable assumptions on their impact on a particular output, this steps refers to the *expert knowledge* that Bayesian methods rely on. Additionally, selecting only most relevant variables for each output helps reduce the dimensionality of the problem, which poses great challenges in the context of often lacking data to validate or inform the model.

The linear model in turn yields a value $\mu_{S_i}$ for a specific sector, which is interpreted as a mean value for a secor in a location by the Bayesian method and is combined with an uncertainty metric $\sigma$ measured before hand as the average availability of data in the system. The tuple of values $(\mu_{S_i}, \sigma)$, with $\sigma$ fixed and $\mu_{S_i}$ which is obtained from the sampled linear combination of the $\alpha$ parameters.

## All together

![](imgs/JPN_model_graph.png)

## Data

::: {.columns layout="[[1,2]]" fontsize="10"}
::: column
### Input[^1]

#### Current

POIs, GHSL NRES, DOSE-WDI, Copernicus, GHSL pop, Bureau of Economic Analysis (BEA), ILOSTAT, UK Business Value Added, EU IO tables

### Validation

-   Kummu
-   Bea
-   EU IO
-   Null model (OLD)

### Subsectors

-   GEM
-   CGFI
-   Climatrace
-   Edgar
-   MAPSPAM
:::

::: column
### Catalogue

Is up on the cluster with access and manipulation facilitated by the [`scalenav`](https://github.com/nismod/scale-nav) package

![](imgs/isic_section_coverage.png)
:::
:::

[^1]: @janssens-maenhout2019; @europeancommission.jointresearchcentre.2023; @gaulier2010; @kummu2018a; @you2014; @wenz2015 ...

<!-- end columns -->

# Let's see some results !

## Selected Regions

::: {.columns layout="[[5,3]]"}
::: column
### FRA.13_1

![](imgs/FRA.13_1_2025_8_29_16_22/BHM_FRA.13_1_h3_6_indus_3_total_map.png){fig-align="center"}
:::

::: column
![](imgs/FRA.13_1_2025_8_29_16_22/BHM_FRA.13_1_h3_6_indus_3_proxy_weights.png) ![](imgs/FRA.13_1_2025_8_29_16_22/BHM_FRA.13_1_h3_6_indus_3_region_output.png)
:::
:::

<!-- end columns -->

## Selected Regions

::: {.columns layout="[[-1,-1],[1,1],[-1,-1]]"}
::: column
![](imgs/FRA.13_1_2025_8_29_16_22/BHM_FRA.13_1_h3_6_indus_3_sectors_map.png){fig-align="center"}
:::

::: column
![](imgs/FRA.13_1_2025_8_29_16_22/BHM_FRA.13_1_h3_6_indus_3_sectors_mean.png)
:::
:::

## Selected Regions

::: {.columns layout="[[-1,-1],[1,1],[-1,-1]]"}
::: column
![](imgs/FRA.13_1_2025_8_29_16_22/BHM_FRA.13_1_h3_6_indus_3_poi_dens_map.png)
:::

::: column
![](imgs/FRA.13_1_2025_8_29_16_22/BHM_FRA.13_1_h3_6_indus_3_prod_vs_poi.png)
:::
:::

<!-- JAPAN -->

## Selected Regions

::: {.columns layout="[[5,3]]"}
::: column
### Japan

![](imgs/JPN_2025_8_29_20_56/BHM_JPN_h3_5_indus_3_sectors_map.png)
:::

::: column
![](imgs/JPN_2025_8_29_20_56/BHM_JPN_h3_5_indus_3_proxy_weights.png) ![](imgs/JPN_2025_8_29_20_56/BHM_JPN_h3_5_indus_3_region_output.png)
:::
:::

<!-- end columns -->

## Selected Regions

::: {.columns layout="[[-1,-1],[1,1],[-1,-1]]"}
::: column
![](imgs/JPN_2025_8_29_20_56/BHM_JPN_h3_5_indus_3_sectors_map.png)
:::

::: column
![](imgs/JPN_2025_8_29_20_56/BHM_JPN_h3_5_indus_3_sectors_mean.png)
:::
:::

## Selected Regions

::: {.columns layout="[[-1,-1],[1,1],[-1,-1]]"}
::: column
![](imgs/JPN_2025_8_29_20_56/BHM_JPN_h3_5_indus_3_poi_dens_map.png)
:::

::: column
![](imgs/JPN_2025_8_29_20_56/BHM_JPN_h3_5_indus_3_prod_vs_poi.png)
:::
:::

## Modelling Diagnostics

The BHM side of things ![](imgs/FRA.13_1_2025_8_29_16_22/BHM_FRA.13_1_h3_6_indus_3_trace_plot.png)

## Validation 1

We use existing available downscaled data sets, in which one of the 2 dimensions (spatial, sectorial) is coarse, and reduce our data to validate at the resolution of the available data. Example with [@kummu2018], where the data is total GDP at a fine spatial scale. We reduce the BHM model data by aggregating each locations output across sectors.

### FRA.13_1

::: columns
::: column
![](imgs/FRA.13_1_2025_8_29_16_22/BHM_FRA.13_1_h3_6_indus_3_validation_kummu_diff_map.png)
:::

::: column
![](imgs/FRA.13_1_2025_8_29_16_22/BHM_FRA.13_1_h3_6_indus_3_validation_kummu_plot.png)
:::
:::

<!-- end columns -->

## Validation 1

### JPN

::: columns
::: column
![](imgs/JPN_2025_8_29_20_56/BHM_JPN_h3_5_indus_3_validation_kummu_diff_map.png)
:::

::: column
![](imgs/JPN_2025_8_29_20_56/BHM_JPN_h3_5_indus_3_validation_kummu_plot.png)
:::
:::

<!-- end columns -->

## Validation 2

::: {.columns layout="[[5,3]]"}
::: column
### Florida

![](imgs/USA.10_1_2025_9_2_15_52/BHM_USA.10_1_h3_5_indus_3_total_map.png)
:::

::: column
![](imgs/USA.10_1_2025_9_2_15_52/BHM_USA.10_1_h3_5_indus_3_proxy_weights.png)

![](imgs/USA.10_1_2025_9_2_15_52/BHM_USA.10_1_h3_5_indus_3_region_output.png)
:::
:::

<!-- end columns -->

## Validation 2

::: {.columns layout="[[-1,-1],[1,1],[-1,-1]]"}
::: column
![](imgs/USA.10_1_2025_9_2_15_52/BHM_USA.10_1_h3_5_indus_3_sectors_map.png)
:::

::: column
![](imgs/USA.10_1_2025_9_2_15_52/BHM_USA.10_1_h3_5_indus_3_sectors_mean.png)
:::
:::

## Validation 2

::: {.columns layout="[[-1,-1],[1,1],[-1,-1]]"}
::: column
![](imgs/USA.10_1_2025_9_2_15_52/BHM_USA.10_1_h3_5_indus_3_poi_dens_map.png)
:::

::: column
![](imgs/USA.10_1_2025_9_2_15_52/BHM_USA.10_1_h3_5_indus_3_prod_vs_poi.png)
:::
:::

## Validation 2

Using national reports on per sector productivity at coarse geographic resolution.

![](imgs/USA.10_1_2025_9_2_14_55/BHM_USA.10_1_h3_4_indus_3_bea_bhm.png)

### Reversing the question

The flexibility of BHM allows us to reverse the use of the finer resolution economic output data and use it as a prior knowledge. This is a work in progress feature that is almost implemented.

# Current challenges, limitations and next steps

## Challenges

We use a mix of methods, benefiting from their strengths, but we also inherit their limitations and challenges.

-   Dimensionality

-   Econ model

    -   Colinearity of proxies
    -   Sensitive to spatial and sectorial resolution

-   BHM

    -   Sampling in very high dimensions
    -   Intricate Diagnostics
    -   Complex formalism
    -   Different software ecosysytem and programming paradigm
    -   Sensitive to spatial and sectorial resolution

-   Data

    -   We are constantly looking for data that could be either used to inform our prior, proxy layers, or validation.

-   Predictive modelling

    -   Requires further specifying what are we predicting ?

## Next Steps

-   Running all regions on the cluster
-   Validation routines using incoming regional/national datasets of different resolutions and dimensions
-   **Methods and data publication**
-   Refinement of the model to the different cases of specific data that comes for some areas

## 

<iframe height='120%' width='120%' src="./imgs/JPN_2025_8_29_20_56/BHM_JPN_h3_5_indus_3_deck_map.html"></iframe>

## References
