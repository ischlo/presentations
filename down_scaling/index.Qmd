---
title: "Interpolating global economic activity data"
subtitle: Preliminary results and methods
execute:
  echo: false
  eval: true
  include: true
ipynb-shell-interactivity: all
keep-ipynb: true
bibliography: references.bib
format:
  revealjs:
    theme: [pres_style.scss,default]
    fig-align: center
    code-fold: false
author:
  - name: Ivann Schlosser
    email: ivann.schlosser@ouce.ox.ac.uk
    url: ischlosser.com
    affiliations:
      - name: Oxford Progamme for Sustainable Infrastructure Systems (OPSIS)
        address: South Parks Road
        postal-code: OX1 3QY
        city: Oxford
---

## Introduction

### Looking at economic activity at the finer scale than what the current data allows

-   Wrangling of economic activity datasets (DOSE, @wenz2023)

-   Data projection on the H3 hierarchical indexed grid

-   Developing the right assumptions about the nature of the observed variables

-   Increasing the resolution under the assumptions, using high resolution proxy layers

-   Implementation of a workflow covering this process: package `scale-nav`

## Vocabulary note

**Resolution** - spatial precision of a value, usually between 1 and some integer value (18 in the case of H3, 32 in the case of S2), a greater value corresponds to a finer resolution, in other words greater spatial precision, lower values are coarser resolutions.

**Cell size** - the area of a grid cell, will usually depend on the resolution, a greater resolution means a smaller cell size, and inversely.

**Cell radius/diameter** - in the case of hexagonal cells, analogous to circle radius/diameter.

ex: H3 grid at resolution 10, the cell diameter is about 200 meter.

**Downscaling** - increasing the spatial resolution, reducing the size of cells we work with.

**Grid** - set of cells at a specific resolution

A value observed across a continuous area of interest can be projected onto a discrete grid, with a certain loss of information.

<!-- difference with rasters hierarchical grid vs normal one. -->

A grid represents a discrete representation of an area of interest, which is generally continuous, therefore having a greater resolution (smaller cell sizes) leads to more accurate values.

Interpolation is a usefool technique to infer intermediate values.

Downscaling, similarly to interpolation, allows infering values at a finer resolution than what is originally available in the data.

**Hierarchical index** - an index method that consistently covers various resultions.

**Parent, children** - relation between cells of a lower resolution and higher one that are contained.

The hierarchical indexation captures both the spatial proximity of cells, and their relations across scales (the hierarchy).

**pycnophylactic** - mass preserving

## Input data

Using an *enhanced* DOSE data set, where a number of gaps have been filled with the World Development Indicator data on the national level.

::: {layout-nrow="2"}
![](img/world_gdp.png)

![](img/world_manuf.png)

![](img/world_agri.png)

![](img/world_services.png)
:::

## Hierarchical indexed grids

::::: columns
::: {.column width="60%"}
### H3

> Developped by Uber

-   original library in C
-   open-source
-   bindings to many other languages/systems
-   hexagonal shapes
-   hierarchical (see @fig-h3_hier)
-   Theory:[@sahr2014]
-   edge indexation (useful to model flows for example), nearest neighbours
-   node indexation
-   the nature of the projection imposes that a constant number of 12 pentagons is present in the index at each level.
:::

::: {.column width="40%"}
![H3 levels](img/parent-child.png){#fig-h3_hier .lightbox fig-align="center" width="100%"}
:::
:::::

::: aside
source: <https://h3geo.org/docs/highlights/indexing>
:::

## Example : Interpolation

<!-- Downscaling a **sinus** function. -->

```{r, fig.dpi=250, fig.width=6,fig.align = 'center'}
# layout-valign="bottom"

######

par(
  mfrow = c(2, 2),
  yaxt = "n",
  mar = c(1, .5, 1.5, .5),
  oma = c(1, 1, 1, 1)
)
# params

step <- 20

x <- seq(-150, 150, by = 1) * pi / 100

noise <- rnorm(n = length(x), mean = 0, sd = .08)

sin_x <- sin(x) + noise

xlab <- "x"
ylab <- "y"
xlim <- c(-pi, pi)
ylim <- c(-1.1, 1.1)
cex <- 0.5


x_step <- seq(-150, 150, by = step) * pi / 100

sin_x_step <- sin(x_step)

####

plot(x_step, sin_x_step,
  type = "p",
  ylab = ylab,
  xlab = xlab,
  col = "darkred", ,
  ylim = ylim,
  xlim = xlim,
  main = "Sample data",
  xaxt = "n"
)
# lines(x, sin_x, type = "l")

#####

plot(x_step - step * pi / 200, sin_x_step,
  type = "s",
  col = "darkred",
  xlab = xlab,
  ylab = ylab,
  xlim = xlim,
  ylim = ylim,
  main = "Step-wise interpolation",
  xaxt = "n"
)
points(x_step, sin_x_step, col = "darkred")
# lines(x, sin_x)

#####

plot(x_step, sin_x_step,
  type = "l",
  xlab = xlab,
  ylab = "",
  col = "darkred",
  ylim = ylim,
  xlim = xlim,
  main = "Linear interpolation"
)
# lines(x, sin_x)
points(x_step, sin_x_step, col = "darkred")

#####

plot(x,
  sin_x,
  type = "l",
  ylab = ylab,
  ylim = ylim,
  xlim = xlim,
  main = "Actual data"
)

####

par(mfrow = c(1, 1))

```

## Example : downscaling

```{r fig.dpi=250, fig.width=6,fig.align = 'center'}

par(
  mfrow = c(2, 2),
  yaxt = "n",
  mar = c(1, .5, 1.5, .5),
  oma = c(1, 1, 1, 1)
)
# params
#
#
# # graphical
# xlab <- "x"
# ylab <- "y"
# xlim <- c(-pi, pi)
# ylim <- c(-1,1)

# plotting variables
# step <- 20
step_down <- 6

# x <- seq(-150, 150, by = 1) * pi / 100
# sin_x <- sin(x)

# x_step <- seq(-150, 150, by = step) * pi / 100

x_step_down <- seq(-150, 150, by = step_down) * pi / 100

sin_step_down <- c()
for (x_ in x_step_down) {
  sin_step_down <- append(sin_step_down, sin(x_step[which.min(abs(x_ - x_step))]))
}
sin_x_step <- sin(x_step)


sin_lin_down <- c()
for (x_ in x_step_down[-length(x_step_down)]) {
  # x_ <- -2.2619467
  x_1 <- x_step[which(x_ >= x_step)[length(which(x_ >= x_step))]]
  x_2 <- x_step[which(x_ < x_step)[1]]
  sin_lin_down <- append(sin_lin_down, sin(x_1) + (x_ - x_1) * (sin(x_2) - sin(x_1)) / (x_2 - x_1))
}
sin_lin_down <- append(sin_lin_down, sin(sin_lin_down[length(x_step_down)]))

####

plot(x_step, sin_x_step,
  type = "p",
  ylab = ylab,
  xlab = xlab,
  col = "darkred", ,
  ylim = ylim,
  xlim = xlim,
  main = "Sample data",
  xaxt = "n"
)

# lines(x, sin_x, type = "l")

#####

plot(x_step - step * pi / 200, sin_x_step,
  type = "s",
  # lty =
  col = "darkred",
  xlab = xlab,
  ylab = ylab,
  xlim = xlim,
  ylim = ylim,
  main = "Step-wise interpolation",
  xaxt = "n"
)
points(x_step, sin_x_step, col = "darkred")
points(x_step_down, sin_step_down, col = "black", pch = 19, cex = cex)
# lines(x, sin_x)

#####

plot(x_step, sin_x_step,
  type = "l",
  xlab = xlab,
  ylab = "",
  col = "darkred",
  ylim = ylim,
  xlim = xlim,
  main = "Linear interpolation"
)
# lines(x, sin_x)
points(x_step, sin_x_step, col = "darkred")
points(x_step_down, sin_lin_down, col = "black", pch = 19, cex = cex)

#####

plot(x,
  sin_x,
  type = "l",
  ylab = ylab,
  ylim = ylim,
  xlim = xlim,
  main = "Actual data"
)

####

par(mfrow = c(1, 1))

```

<!-- ## Resolution -->

<!-- ## lsdkfm -->

<!-- Interpolation is a method for infering values of a variable across a region from a limited set of measurements of that variable in the region. A number of assumptions are usually made in an interpolation process, most commonly the continuity of the modelled variable. -->

<!-- Downscaling and interpolation can be seen as joint techniques -->

<!-- TODO:  -->

<!-- Brief explanation of why both go hand in hand, examples of interpolation techniques . -->

<!-- How to apply them in downscaling -->

<!-- infering downscaled values from interpolated values. A way to do an 'educated' guess. -->

<!-- In our case, use most granular data avilable as one of the inputs and proxies for the donscaling of other values -->

## Binning and donwscaling {visibility="hidden"}

```{r fig.dpi=250, fig.width=6,fig.align = 'center'}

n <- 1500

x <- runif(n = n, min = -2, max = 2)
# ,mean = 0,sd = .5)

x_norm <- rnorm(n = n, mean = 0, sd = 1) + x

step_ <- c(1, 7, 14)

par(
  mfrow = c(2, 2),
  yaxt = "n",
  mar = c(1, .5, 1.5, .5),
  oma = c(1, 1, 1, 1)
)

for (i in step_) {
  hist(x_norm,
    breaks = i,
    xlab = "x",
    ylab = "",
    freq = FALSE,
    main = paste0(i + 1, " bins counting"),
    xlim = xlim,
    xaxt = ifelse(i == step_[length(step_)], "s", "n")
  )
}

density(x_norm) |> plot(
  xlab = "x",
  ylab = "",
  xlim = xlim,
  main = "continuous",
  bty = "n"
)

par(mfrow = c(1, 1))

```

## Data tranformation

### Rescaling approach

Consists in updating the finer scale values from a known distribution at the given resolution. Using the average in the trivial case. This method ensures that the scale change is mass preserving (pycnophylactic).

::::: columns
::: {.column width="50%"}
### Decreasing resolution
:::

::: {.column width="50%"}
### Increasing resolution
:::
:::::

### Extensive case

::::: columns
::: {.column width="50%"}
$$
W(H_i) = \sum_{j=0..6}w(h_{ij})
$$ It adds up.
:::

::: {.column width="50%"}
$$
w(h_{ij}) = g_W(h_{ij})W(H_i)
$$ such that $\sum_j g_W(h_{ij}) = 1$ is a density distribution associated to the variable $W$ inside location $H_i$ at higher resolution, that can be taken from external proxy data. In the trivial case, $g_W\equiv \frac{1}{N_c}$
:::
:::::

### Intensive case

::::: columns
::: {.column width="50%"}
$$
W(H_i)= \frac{\sum_jv_{ij}}{\sum_jq_{ij}}
$$

using the associated extensive variables $v,q$.

<!-- \sum_{j=0..6}g_w(h_{ij})w(h_{ij}) -->
:::

::: {.column width="50%"}
$$
w(h_{ij}) = W(H_i)
$$

in the trivial case. A more advanced approach would be to consider the associated extensive variables. If $W_{i}=\frac{V_{i}}{Q_{i}}$, then $w_{ij}=\frac{V_i g_{V}(h_{ij})}{Q_ig_Q(h_{ij})}$. Example: $GDP/cap$.
:::
:::::

## In practice : simple example

::::: columns
::: {.column width="50%"}
A parent cell $H_i^{(n)}$ with resolution $n$ contains a set of children $\{h^{(n+1)}_{ij}\}_{j=0...6}$ with resolution $n+1$. A value $W$ *measured* at $H_i^{(n)}$, $W(H)$ is downscaled into $w(h)$.

### Trivial rescaled case

For an extensive variable, if $W(H^n_i)=21$, we can downscale into $$w(h_{ij})=\frac{W(H^n_i)}{N_c}=\frac{21}{7}=3$$ for all j.

### General rescaled case

If, we have a known underlying distribution at a finer scale, say $g_W(h_{ij}) = 18/105$ for $j=\{0,1,4,5,6\}$, and $g_W(h_{i2})=1/21,g_W(h_{i3})=2/21$, we incorporate this data into the equation and get:

$$
w(h_{ij})=g_W(h_{ij})W(H_i) = 
\begin{cases}
    3.6,& \text{for } j=0,1,4,5,6 \\
    1,              & \text{for } j=2\\
    2, & \text{for } j=3\\
\end{cases}
$$ for each children cell.
:::

::: {.column width="50%"}
![source : https://h3geo.org/](img/hex_id.png){fig-align="center"}
:::
:::::

## In practice : GDP + Non-res buildings data

For a certain level (Admin0,Admin1), we know the GDP of an area (in USD) for given sectors. We know the amount of non-residential infrastructure in the region at a fine resolution (100x100m grid). We use assume a typical value of gdp generated for a unit a infrastructure and rescale to obtain a density layer of infrastrucure for the given area. Combine with the known ecnomic output at a coarser scale to downscale it.

::: {.r-stack .center}
![](img/gridded_zoom.png){.fragment fig-align="center" width="70%"}

![](img/gridded_zoom-2.png){.fragment fig-align="center" width="70%"}

![](img/gridded_zoom-3.png){.fragment fig-align="center" width="70%"}

![](img/gridded_zoom-5.png){.fragment fig-align="center" width="70%"}
:::

## Further

::: {.r-stack .center}
![](img/gridded.png){.fragment fig-align="center" width="70%"}

![](img/gridded-2.png){.fragment fig-align="center" width="70%"}

![](img/gridded-3.png){.fragment fig-align="center" width="70%"}

![](img/gridded-4.png){.fragment fig-align="center" width="70%"}
:::

## In Practice 2: Agriculture yield + constraint layer

We have a raster grid with agricultural yields from **MAPSPAM**. The raster grid cell size is $10km\times 10km$. We can try to downscale this while taking into consideration the human settlements that are covered by the grid using the **GHSL** total built up ground surface. By removing the cells that are covered in infrastructure, we dowscale the agriculture layer by taking into consideration the constrain on where crops are more likely to be in fact cultivated.

::: {.r-stack .center}
![](img/grid_unconstrained_zoom.png){.fragment fig-align="center" width="70%"}

![](img/grid_constrained_zoom.png){.fragment fig-align="center" width="70%"}
:::

## Two directions

In any case, we rely on high resolution proxy/constrain layers and low resolution data layers.

::::: columns
::: {.column width="50%"}
### Top-bottom

From low resolution (parent cells) to high resolution (descendant cells)

-   A low resolution data layer is downscaled into high resolution with the use of appropriate high resolution proxy layers.
:::

::: {.column width="50%"}
### Bottom-up

From high resolution (descendant cells) to lower resolution (parent cells)

-   A high resolution proxy/constraint layer is upscaled to aggregate it at a lower resolution.
:::
:::::

## Workflow

![](img/flowchart.png){fig-align="center" width="100%"}

## Conclusion

### Challenges

Lots of challenges both on the conceptual understanding of what is going on and the right choice of base data layers, constraints etc...

> Question for the audience : good proxy and constraint data layers ?

But also technical implementation as the number of cells grows exponentially with each level of downscaling and all the data sizes with it.

> Currently using duckDB with ibis and their geospatial extensions.

### Next steps

-   Robust workflows to project all sorts of spatial data onto the grid with the *pycnophylactic* principle.

<!-- what proxy to differentiate industry ... -->

<!-- constraints ? -->

## Literature

@tobler1979

::::: columns
::: {.column width="50%"}
> pycnophylactic : mass preserving
:::

::: {.column width="50%"}
![](img/bivariate_discrete_tobbler.png){fig-align="center" width="100%"}
:::
:::::

## Online

### Gap filling DOSE

## Literature

@roudier2017

@bürger2012;

@giuliani2022;

@malone2012;

@vrac2007;

@schoof2013;

@frías2006;

@murakami2019;

@khan2006;

@ekström2015

## Literature

### Contemporary methods: DL

@gonzalez2022

python package: [dl4ds](https://carlos-gg.github.io/dl4ds/dl4ds.html)

![source : https://carlos-gg.github.io/dl4ds/dl4ds.html](img/dl4ds.png){fig-align="center" width="60%"}

## Online material

### Packages

**R**

-   [`pyconphy`](https://rdrr.io/github/rspatial/predicts/man/pycnophy.html): pycnophylactic interpolation
-   [`dissever`](https://github.com/pierreroudier/dissever)
    -   [`caret`](https://topepo.github.io/caret/)
    -   [`gam`](https://m-clark.github.io/generalized-additive-models/application.html)
    -   [`randomForest`](https://www.r-bloggers.com/2021/04/random-forest-in-r/)
    -   [`Cubist`](https://topepo.github.io/Cubist/index.html)
    -   [`gstat`](https://r-spatial.github.io/gstat/)
-   [`ClimDown`](https://github.com/pacificclimate/ClimDown)
-   [LULCdown](https://github.com/ggiuliani/LULCdown?tab=readme-ov-file)
-   [downscaleR](https://github.com/SantanderMetGroup/downscaleR)
-   [CDT](https://iri.columbia.edu/~rijaf/CDTUserGuide/html/cdt_file_menu.html)
-   [exactextractr](https://isciences.gitlab.io/exactextractr/)

## References